{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "\n",
    "At first, we will generate synthetic data based on a linear relationship and visualize it using a scatter plot. We will also add Gaussian noise to the data to simulate real-world scenarios. \n",
    "\n",
    "Afterwards, we will try to fit a (linear) model to the data and check if we can reconstruct/uncover the true underlying model parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Model Function Description\n",
    "\n",
    "The `linear_model` function predicts the dependent variable $y$ from the independent variable $x$ based on a linear relationship. It uses the equation $y = \\theta_1 x + \\theta_0$, where $\\theta_1$ is the slope and $\\theta_0$ is the intercept of the line.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the linear model function to predict y from x\n",
    "def linear_model(x, theta_1, theta_0):\n",
    "    \"\"\"\n",
    "    Linear model to predict y from x based on a linear relationship.\n",
    "    \n",
    "    Parameters:\n",
    "    - x (array-like): Independent variable.\n",
    "    - theta_1 (float): Slope of the line.\n",
    "    - theta_0 (float): Intercept of the line.\n",
    "    \n",
    "    Returns:\n",
    "    - y (array): Predicted values of the dependent variable.\n",
    "    \"\"\"\n",
    "    return theta_1 * x + theta_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Generate Synthetic Data\n",
    "# True parameters for the linear relationship (usually unknown in practice)\n",
    "theta_1 = 3   # Slope\n",
    "theta_0 = 4   # Intercept\n",
    "\n",
    "# Generate some x values\n",
    "x = np.random.uniform(0, 10, 50)\n",
    "\n",
    "print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compute y values without noise\n",
    "y_true = linear_model(x, theta_1, theta_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data\n",
    "plt.scatter(x, y_true, color=\"blue\", label=\"Data points\")\n",
    "plt.xlabel(\"$x$\")\n",
    "plt.ylabel(\"$y$\")\n",
    "plt.title(\"Scatter Plot of Synthetic Data without Noise\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate external Gaussian noise\n",
    "noise_level = 2  # Standard deviation of the noise\n",
    "noise = np.random.normal(0, noise_level, x.shape)\n",
    "\n",
    "print(noise)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the noise\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(noise, color='red', marker='o', linestyle='None')\n",
    "plt.title('Random Noise')\n",
    "plt.xlabel('Sample')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Add noise to y\n",
    "y = y_true + noise\n",
    "\n",
    "# Plot the data\n",
    "plt.scatter(x, y, color=\"blue\", label=\"Data points\")\n",
    "plt.plot(x, y_true, color=\"green\", linestyle=\"--\", label=f\"True line: $y = {theta_1}x + {theta_0}$\")\n",
    "plt.xlabel(\"$x$\")\n",
    "plt.ylabel(\"$y$\")\n",
    "plt.title(\"Scatter Plot of Synthetic Data with Noise\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model: Estimating Parameters\n",
    "\n",
    "In this section, we will estimate the parameters of our linear model, namely the slope ($\\theta_1$) and the intercept ($\\theta_0$). This process is often referred to as \"training\" the model. We will use the least squares method to find the best-fitting line to our noisy data. For more complicated models, other methods are used to estimate the model parameters. The goal is to minimize the sum of the squared differences between the observed values and the values predicted by our model. Good that we can calculate that analytically :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression using Ordinary Least Squares (OLS)\n",
    "\n",
    "Linear regression is a method to model the relationship between a dependent variable $y$ and one or more independent variables $x$. The goal is to find the best-fitting line through the data points.\n",
    "\n",
    "## Ordinary Least Squares (OLS)\n",
    "\n",
    "OLS is a method to estimate the parameters $\\theta_0$ and $\\theta_1$ of the linear regression model. The linear regression model can be written as:\n",
    "\n",
    "$$ y_i = \\theta_0 + \\theta_1 x_i + \\epsilon_i $$\n",
    "\n",
    "where:\n",
    "- $y_i$ is the $i$-th observed dependent variable.\n",
    "- $x_i$ is the $i$-th observed independent variable.\n",
    "- $\\theta_0$ is the intercept.\n",
    "- $\\theta_1$ is the slope.\n",
    "- $\\epsilon_i$ is the error term for the $i$-th observation.\n",
    "\n",
    "The OLS method minimizes the sum of the squared residuals (the differences between the observed and predicted values). The objective function to minimize is:\n",
    "\n",
    "$$ L(\\theta_0, \\theta_1) = \\sum_{i=1}^{n} (y_i - (\\theta_0 + \\theta_1 x_i))^2 $$\n",
    "\n",
    "## Analytical Solution\n",
    "\n",
    "To find the optimal values of $\\theta_0$ and $\\theta_1$, we need to take the partial derivatives of $L(\\theta_0, \\theta_1)$ with respect to $\\theta_0$ and $\\theta_1$, and set them to zero.\n",
    "\n",
    "### Partial Derivative with respect to $\\theta_0$\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial \\theta_0} = \\frac{\\partial}{\\partial \\theta_0} \\sum_{i=1}^{n} (y_i - (\\theta_0 + \\theta_1 x_i))^2 $$\n",
    "\n",
    "Using the chain rule:\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial \\theta_0} = \\sum_{i=1}^{n} 2(y_i - (\\theta_0 + \\theta_1 x_i))(-1) $$\n",
    "\n",
    "Simplifying:\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial \\theta_0} = -2 \\sum_{i=1}^{n} (y_i - \\theta_0 - \\theta_1 x_i) $$\n",
    "\n",
    "Setting the derivative to zero:\n",
    "\n",
    "$$ -2 \\sum_{i=1}^{n} (y_i - \\theta_0 - \\theta_1 x_i) = 0 $$\n",
    "\n",
    "Dividing by -2:\n",
    "\n",
    "$$ \\sum_{i=1}^{n} (y_i - \\theta_0 - \\theta_1 x_i) = 0 $$\n",
    "\n",
    "Rearranging:\n",
    "\n",
    "$$ \\sum_{i=1}^{n} y_i = n \\theta_0 + \\theta_1 \\sum_{i=1}^{n} x_i $$\n",
    "\n",
    "Solving for $\\theta_0$:\n",
    "\n",
    "$$ \\theta_0 = \\frac{\\sum_{i=1}^{n} y_i - \\theta_1 \\sum_{i=1}^{n} x_i}{n} $$\n",
    "\n",
    "### Partial Derivative with respect to $\\theta_1$\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial \\theta_1} = \\frac{\\partial}{\\partial \\theta_1} \\sum_{i=1}^{n} (y_i - (\\theta_0 + \\theta_1 x_i))^2 $$\n",
    "\n",
    "Using the chain rule:\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial \\theta_1} = \\sum_{i=1}^{n} 2(y_i - (\\theta_0 + \\theta_1 x_i))(-x_i) $$\n",
    "\n",
    "Simplifying:\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial \\theta_1} = -2 \\sum_{i=1}^{n} (y_i - \\theta_0 - \\theta_1 x_i)x_i $$\n",
    "\n",
    "Setting the derivative to zero:\n",
    "\n",
    "$$ -2 \\sum_{i=1}^{n} (y_i - \\theta_0 - \\theta_1 x_i)x_i = 0 $$\n",
    "\n",
    "Dividing by -2:\n",
    "\n",
    "$$ \\sum_{i=1}^{n} (y_i - \\theta_0 - \\theta_1 x_i)x_i = 0 $$\n",
    "\n",
    "Rearranging:\n",
    "\n",
    "$$ \\sum_{i=1}^{n} y_i x_i = \\theta_0 \\sum_{i=1}^{n} x_i + \\theta_1 \\sum_{i=1}^{n} x_i^2 $$\n",
    "\n",
    "Substituting $\\theta_0$ from the previous equation:\n",
    "\n",
    "$$ \\sum_{i=1}^{n} y_i x_i = \\left( \\frac{\\sum_{i=1}^{n} y_i - \\theta_1 \\sum_{i=1}^{n} x_i}{n} \\right) \\sum_{i=1}^{n} x_i + \\theta_1 \\sum_{i=1}^{n} x_i^2 $$\n",
    "\n",
    "Simplifying:\n",
    "\n",
    "$$ \\sum_{i=1}^{n} y_i x_i = \\frac{\\sum_{i=1}^{n} y_i \\sum_{i=1}^{n} x_i}{n} - \\theta_1 \\frac{(\\sum_{i=1}^{n} x_i)^2}{n} + \\theta_1 \\sum_{i=1}^{n} x_i^2 $$\n",
    "\n",
    "Solving for $\\theta_1$:\n",
    "\n",
    "$$ \\theta_1 = \\frac{n \\sum_{i=1}^{n} y_i x_i - \\sum_{i=1}^{n} y_i \\sum_{i=1}^{n} x_i}{n \\sum_{i=1}^{n} x_i^2 - (\\sum_{i=1}^{n} x_i)^2} $$\n",
    "\n",
    "Finally, substituting $\\theta_1$ back into the equation for $\\theta_0$:\n",
    "\n",
    "$$ \\theta_0 = \\frac{\\sum_{i=1}^{n} y_i - \\theta_1 \\sum_{i=1}^{n} x_i}{n} $$\n",
    "\n",
    "## Using Mean, Variance, and Covariance\n",
    "\n",
    "We can express the above formulas using the mean, variance, and covariance of the data.\n",
    "\n",
    "- Mean of $x$: $\\bar{x} = \\frac{1}{n} \\sum_{i=1}^{n} x_i$\n",
    "- Mean of $y$: $\\bar{y} = \\frac{1}{n} \\sum_{i=1}^{n} y_i$\n",
    "- Variance of $x$: $\\text{Var}(x) = \\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\bar{x})^2$\n",
    "- Covariance of $x$ and $y$: $\\text{Cov}(x, y) = \\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})$\n",
    "\n",
    "Using these definitions, we can rewrite the formulas for $\\theta_0$ and $\\theta_1$ as:\n",
    "\n",
    "$$ \\theta_1 = \\frac{\\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})}{\\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\bar{x})^2} = \\frac{\\text{Cov}(x, y)}{\\text{Var}(x)}$$\n",
    "\n",
    "$$ \\theta_0 = \\bar{y} - \\theta_1 \\bar{x} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Calculate the Linear Regression Parameters θ_1 and θ_0\n",
    "# Using the least squares method to solve for θ_1 (slope) and θ_0 (intercept)\n",
    "\n",
    "# Compute means of x and y\n",
    "x_mean = np.mean(x)\n",
    "y_mean = np.mean(y)\n",
    "\n",
    "# Calculate θ_1 (slope)\n",
    "numerator = np.sum((x - x_mean) * (y - y_mean))\n",
    "denominator = np.sum((x - x_mean) ** 2)\n",
    "theta_1_estimated = numerator / denominator\n",
    "\n",
    "# Calculate θ_0 (intercept)\n",
    "theta_0_estimated = y_mean - theta_1_estimated * x_mean\n",
    "\n",
    "print(f\"Estimated parameters:\")\n",
    "print(f\"θ_1 (slope) = {theta_1_estimated:.2f}\")\n",
    "print(f\"θ_0 (intercept) = {theta_0_estimated:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = linear_model(x, theta_1_estimated, theta_0_estimated)\n",
    "\n",
    "# Plot the data and the regression line\n",
    "plt.scatter(x, y, color=\"blue\", label=\"Data points\")\n",
    "plt.plot(x, y_hat, color=\"red\", label=\"Fitted line: $\\hat{y} = \\\\theta_1 \\cdot x + \\\\theta_0$\")\n",
    "# plt.plot(x, y_true, color=\"green\", label=\"True line: $y = \\\\theta_1 \\cdot x + \\\\theta_0$\")\n",
    "plt.xlabel(\"$x$\")\n",
    "plt.ylabel(\"$y$\")\n",
    "plt.title(\"Linear Regression Fit\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Calculate the residuals (errors)\n",
    "# Error (residual) for each point: ε = |ŷ - y|\n",
    "errors = np.abs(y - y_hat)\n",
    "\n",
    "# Mean Absolute Error (MAE) as an overall error metric\n",
    "mae = np.mean(errors)\n",
    "print(f\"Mean Absolute Error (MAE) = {mae:.2f}\")\n",
    "\n",
    "# Plot the residuals\n",
    "plt.scatter(x, errors, color=\"purple\", label=\"Residuals |$\\hat{y} - y$|\")\n",
    "plt.axhline(y=mae, color=\"gray\", linestyle=\"--\", label=f\"Mean Absolute Error (MAE) = {mae:.2f}\")\n",
    "plt.xlabel(\"$x$\")\n",
    "plt.ylabel(\"Residuals |$\\\\epsilon$|\")\n",
    "plt.title(\"Residuals of the Linear Regression Model\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ki2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
